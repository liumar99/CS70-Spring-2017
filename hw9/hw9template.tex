\documentclass[11pt]{article}

\usepackage{amsmath,amssymb,amsthm,setspace,tabto,fancyhdr,sectsty,mathtools}
\usepackage[shortlabels]{enumitem}
\usepackage[nobreak=true]{mdframed}
\usepackage[left=1.25in,right=0.75in,top=1.25in,bottom=2.0in]{geometry}

\newcommand*{\Question}[1]{\section{#1}}
\newenvironment{Parts}{\begin{enumerate}[label=(\alph*)]}{\end{enumerate}}
\newcommand*{\Part}{\item}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% UNCOMMENT NEXT LINE FOR SOLUTION BOXES
 \newcommand*{\solnboxes}{}

%%%%%%%%%%%%%%%%%%%% name/id
\rfoot{\small Andy Zhang | 3031931319}

%%%%%%%%%%%%%%%%%%%% hw number
\newcommand*{\hwnum}{9}


\ifdefined\solnboxes
    \newenvironment{Answer}{\vspace{10pt}\begin{mdframed}\textbf{Solution}\\}{\end{mdframed}\vfill\pagebreak[3]}
\else
    \newenvironment{Answer}{\vspace{10pt}}{\vfill\pagebreak[3]}
\fi
\newcommand*{\MC}[1]{\multicolumn{1}{c}{#1}}
\newcommand*{\N}{\mathbb{N}}
\newcommand*{\Z}{\mathbb{Z}}
\newcommand*{\Q}{\mathbb{Q}}
\newcommand*{\R}{\mathbb{R}}
\newcommand*{\C}{\mathbb{C}}
\newcommand*{\GF}{\text{GF}}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\pagestyle{fancy}
\headheight=75pt
\sectionfont{\Large\fontfamily{lmdh}\selectfont}

\renewcommand{\headrulewidth}{6pt}
\chead{\rule{\textwidth}{6pt} \vspace{20pt}\\}
\lhead{\setstretch{1.05}\Large\fontfamily{lmdh}\selectfont
CS 70        \tabto{96pt} Discrete Mathematics and Probability Theory\smallskip\\
Spring 2017  \tabto{96pt} Rao}
\rhead{\huge     \fontfamily{lmdh}\selectfont     HW \hwnum}

\lfoot{\small CS 70, Spring 2017, HW \hwnum}
\begin{document}

\Question{Sundry} 
\vspace{10pt}
%%%%%%%%%%%%%%%%%%%% SUNDRY PART HERE
\noindent Nadir Akhtar: nadir\_akhtar@berkeley.edu\\
Rustie Lin: rustielin@berkeley.edu\\
Sukrit Arora: sukrit.arora@berkeley.edu\\

I certify that all solutions are entirely in my words and that I have not looked at another studentâ€™s
solutions. I have credited all external sources in this write up. - Andy Zhang
%%%%%%%%%%%%%%%%%%%% END SUNDRY
\vfill\pagebreak[3]

%%%%%%%%%%%%%%%%%%%% QUESTIONS START HERE
\Question{Cliques in Random Graphs}

Consider a graph $G(V,E)$ on $n$ vertices which is generated by the following random process: for each pair of vertices $u$ and $v$, we flip a fair coin and place an (undirected) edge between $u$ and $v$ if and only if the coin comes up heads. So for example if $n = 2$, then with probability $1/2$, $G(V,E)$ is the graph consisting of two vertices connected by an edge, and with probability $1/2$ it is the graph consisting of two isolated vertices.

\begin{Parts}
	\Part What is the size of the sample space?
	\begin{Answer}
        $2^{\binom{n}{2}}$
	\end{Answer}

	\Part A $k$-clique in graph is a set of $k$ vertices which are pairwise adjacent (every pair of vertices is connected by an edge). For example a $3$-clique is a triangle. What is the probability that a particular
	set of $k$ vertices forms a $k$-clique? 
	\begin{Answer}
    $\frac{1}{2^\binom{k}{2}}$
	\end{Answer}
	\Part Prove that the probability that the graph contains a $k$-clique for $k = 4\ceil{\log n}+1$ is at most
	$1/n$. 
	\begin{Answer}
    Since there are $\binom{n}{k}$ ways to choose a $k$-clique, we know from part $(b)$ that there is a $\frac{\binom{n}{k}}{2^\binom{k}{2}}$ probability that the graph contains a $k$-clique. From this value, we get the following equation:$$\frac{\binom{n}{k}}{2^{\frac{k(k-1)}{2}}} \leq \frac{n^k}{2^{\frac{k(k-1)}{2}}} \leq \frac{n^k}{2^{2k\log(n)}}=\frac{n^k}{n^{2k}}=\frac{1}{n^k}\leq \frac{1}{n}$$ Thus, when $k = 4\ceil{\log n}+1$, the probability that the graph contains a $k$-clique is at most $1/n$
	\end{Answer}
\end{Parts}

\Question{Student Request Collector} 

After a long night of debugging, Alvin has just perfected the new homework party/office hour queue system. CS 70 students sign themselves up for the queue, and TAs go through the queue, resolving requests one by one. Unfortunately, our newest TA (let's call him TA Bob) does not understand how to use the new queue: instead of resolving the requests in order, he always uses the Random Student button, which (as the name suggests) chooses a random student in the queue for him. To make matters worse, after helping the student, Bob forgets to click the Resolve button, so the student still remains in the queue! For this problem, assume that there are $n$ total students in the queue.
\begin{Parts}
	\Part Suppose that Bob has already helped $k$ students. What is the probability that the Random Student button will take him to a student who has not already been helped?
	\begin{Answer}
    $\frac{n-k}{n}$
	\end{Answer}

	\Part Let $X_i^r$ be the event that TA Bob has not helped student $i$ after pressing the Random Student button a total of $r$ times. What is $\Pr[X_i^r]$? Assume that the results of the Random Student button are independent of each other. Now approximate the answer using the inequality $1-x \leq e^{-x}$.
	\begin{Answer}
    $$Pr[X^r_i]=(\frac{n-1}{n})^r$$
    $$(\frac{n-1}{n})^r=(1-\frac{1}{n})^r \leq e^{-r/n}$$
	\end{Answer}

	\Part Let $T_r$ represent the event that TA Bob presses the Random Student button $r$ times, but still has not been able to help all $n$ students. (In other words, it takes TA Bob longer than $r$ Random Student button presses before he manages to help every student). What is $T_r$ in terms of the events $X_i^r$? (\textit{Hint}: Events are subsets of the probability space $\Omega$, so you should be thinking of set operations...)
	\begin{Answer}
	$T_r=\bigcup_{i=1}^n X^r_i$
	\end{Answer}

	\Part Using your answer for the previous part, what is an upper bound for $\Pr[T_r]$? (You may leave your answer in terms of $\Pr[X_i^r]$. Use the inequality $1-x \leq e^{-x}$ from before.)
	\begin{Answer}
    $\Pr[T_r]\leq n\Pr[X^r_i]=n(1-\frac{1}{n})^r\leq ne^{-r/n}$
	\end{Answer}

	\Part Now let $r = \alpha n \ln n$. What is $\Pr[X_i^r]$?
	\begin{Answer}
    $n(1-\frac{1}{n})^{\alpha n \ln n}$
	\end{Answer}

	\Part Calculate an upper bound for $\Pr[T_r]$ using the same value of $r$ as before. (This is more formally known as a bound on the tail probability of the distribution of button presses required to help every student. This distribution will be explored in more detail later, in the context of random variables.)
	\begin{Answer}
    $ne^{-\alpha\ln}$
	\end{Answer}

	\Part What value of $r$ do you need to bound the tail probability by $1/n^2$? In other words, how many button presses are needed so that the probability that TA Bob has not helped every student is at most $1/n^2$?
	\begin{Answer}
    $$ne^{-r/n} \leq \frac{1}{n^2}$$
    $$\ln(\frac{1}{n^3}) \leq \frac{-r}{n}$$
    $$r \leq -n\ln(n^{-3})$$
    $$r \leq 3n\ln(n)$$
	\end{Answer}
\end{Parts}


\Question{Combinatorial Coins}

Allen and Alvin are flipping coins for fun. Allen flips a fair coin $k$ times and Alvin flips $n-k$ times. In total there are $n$ coin flips. 

\begin{Parts}
	\Part Use a combinatorial proof to show that $$\sum_{i=0}^k \binom{k}{k - i} \binom{n - k}{i} = \binom{n}{k}.$$
	\begin{Answer}
        The RHS indicates the number of ways in which you can pick $k$ items at a time from a set of $n$ items. The LHS indicates every possible way in which we first split the $n$ items into two groups, one with $k$ items and another with $n-k$ items. Thus, to get every way in which we can choose $k$ items, we sum for all $i$ the ways in which we first pick $k-i$ items from the first group and $i$ items from the second group.
	\end{Answer}

	\Part Prove that the probability that Allen and Alvin flip the same number of heads is equal to the probability that there are a total of $k$ heads.
	\begin{Answer}
    Let $X$ be the binomial random variable representing the number of Allen's heads, let $Y$ be the binomial random variable representing the number of Alvin's heads. Then the probability that $X=i$ and $Y=i$ is the following:
        $$\sum_{i=0}^k\binom{k}{i}\frac{1}{2}^k\binom{n-k}{i}\frac{1}{2}^{n-k}$$
        $$=\frac{1}{2}^n\sum_{i=0}^k\binom{k}{k-i}\binom{n-k}{i}$$
        $$=\frac{1}{2}^n\binom{n}{k}$$
        Since $\frac{1}{2}^n\binom{n}{k}$ indicates the binomial distribution of getting $k$ heads in $n$ coin flips, it is indeed true that the probability that Allen and Alvin flip the same number of heads is equal to the probability that there are a total of $k$ heads.
	\end{Answer}
\end{Parts}


\Question{Geometric Distribution}

Two faulty machines, $M_1$ and $M_2$, are repeatedly run synchronously in parallel (i.e., both machines execute one run, then both execute a second run, and so on). On each run, $M_1$ fails with probability $p_1$ and $M_2$ fails with probability $p_2$, all failure events being independent. Let the random variables $X_1$, $X_2$ denote the number of runs until the first failure of $M_1$, $M_2$ respectively; thus $X_1$, $X_2$ have geometric distributions with parameters $p_1$, $p_2$ respectively. Let $X$ denote the number of runs until the first failure of \emph{either} machine. Show that $X$ also has a geometric distribution, with parameter $p_1+p_2-p_1p_2$.

\begin{Answer}
Since all failure events are independent, and the events of $M_1$ failing and $M_2$ not failing, $M_2$ failing and $M_1$ not failing, and $M_1$ and $M_2$ both failing are disjoint, the distribution of $X$ is as follows:
$$p_1(1-p_1)^{i-1}(1-p_2)^i+p_2(1-p_2)^{i-1}(1-p_1)^i+p_1p_2((1-p_1)(1-p_2))^{i-1}$$
$$=((1-p_1)(1-p_2))^{i-1}(p_1(1-p_2)+p_2(1-p_1)+p_1p_2)$$
$$=(1-(p_1+p_2-p_1p_2))^{i-1}(p_1+p_2-p_1p_2)$$

Thus, $X$ is a geometric distribution with parameter $p_1+p_2-p_1p_2$
\end{Answer}


\Question{Poisson Distribution}

\begin{Parts}
	\Part  It is fairly reasonable to model the number of customers entering a shop during a particular hour as a Poisson random variable. Assume that this Poisson random variable $X$ has mean $\lambda$. Suppose that whenever a customer enters the shop they leave the shop without buying anything with probability $p$. Assume that customers act independently, i.e.~you can assume that they each simply flip a biased coin to decide whether to buy anything at all. Let us denote the number of customers that buy something as $Y$ and the number of them that do not buy anything as $Z$ (so $X = Y+Z$). 
	What is the probability that $Y=k$ for a given $k$? How about $\Pr[Z=k]$? Prove that $Y$ and $Z$ are Poisson random variables themselves.

	\textit{Hint}: You can use the identity
	\begin{align*}
	    e^x=\sum_{k=0}^{\infty}\frac{x^k}{k!}.
	\end{align*}
	\begin{Answer}
        To model every case in which $Y=k$, we must observe when $X=k+k'$ where $k'$ represents the number of people who enter the store but do not buy anything:
        $$\Pr[Y=k]=\sum_{k'=0}^{\infty} \Pr[X=k+k']\Pr[Y=k|X=k+k']$$
        $$=\sum_{k'=0}^{\infty}(\frac{\lambda^{k+k'}}{(k+k')!}e^{-\lambda})(\binom{k+k'}{k}p^{k'}(1-p)^k)$$
        $$=\sum_{k'=0}^{\infty}(\frac{\lambda^{k+k'}}{(k+k')!}e^{-\lambda})(\frac{(k+k')!}{k!k'!}p^{k'}(1-p)^k)$$
	    $$=\frac{(\lambda(1-p))^k}{k!}e^{-\lambda}\sum_{k'=0}^{\infty}(\frac{(\lambda p)^{k'}}{k'!})$$
	    $$=\frac{(\lambda(1-p))^k}{k!}e^{-\lambda}e^{\lambda p}$$
	    $$=\frac{(\lambda(1-p))^k}{k!}e^{-\lambda(1-p)}$$
	    Thus, $Y$ is a Poisson random variable with parameter $\lambda (1-p)$. We can apply the same process for $Pr[Z=k]$ to show that $Z$ is a Poisson random variable with parameter $\lambda p$
	\end{Answer}

	\Part Prove that $Y$ and $Z$ are independent.
	\begin{Answer}
        $$\Pr[Y=k, Z=k']=\Pr[X=k+k']\Pr[Y=k|X=k+k']$$
        $$=(\frac{\lambda^{k+k'}}{(k+k')!}e^{-\lambda})(\frac{(k+k')!}{k!k'!}p^{k'}(1-p)^k)$$
        $$=\frac{(\lambda(1-p))^k}{k!}e^{\lambda(1-p)}\frac{(\lambda(p))^{k'}}{k'!}e^{\lambda(p)}$$
        
        From part (a), we know that this equality simplifies to $\Pr[Y=k]\Pr[Z=k']$. Thus, $Y$ and $Z$ are independent. 
	\end{Answer}

	\Part Assume that you were given two independent Poisson random variables $X_1, X_2$. Assume that the first has mean $\lambda_1$ and the second has mean $\lambda_2$. Prove that $X_1+X_2$ is a Poisson random variable with mean $\lambda_1+\lambda_2$.

	\textit{Hint}: Recall the binomial theorem.
	\begin{align*}
	    (x + y)^n = \sum_{k=0}^n \binom{n}{k} x^k y^{n-k}
	\end{align*}
	\begin{Answer}
    $$\Pr[X_1+X_2=k]=\sum_{i=0}^k\Pr[X_1=k-i]\Pr[X_2=i]$$
    $$=\sum_{i=0}^k\frac{e^{-\lambda_1}\lambda_1^{k-i}}{(k-i)!}\frac{e^{-\lambda_2}\lambda_2^i}{i!}$$
    $$=\frac{e^{-(\lambda_1+\lambda_2)}}{k!}\sum_{i=0}^k\binom{k}{i}\lambda_1^{k-i}\lambda_2^i$$
    $$=\frac{e^{-(\lambda_1+\lambda_2)}(\lambda_1+\lambda_2)^k}{k!}$$
    Thus, $X_1+X_2$ is a Poisson random variable with mean $\lambda_1+\lambda_2$.
	\end{Answer}
\end{Parts}


\Question{Poisson Coupling}

Consider the following discrete joint distribution for $p \in [0, 1]$.
\begin{align*}
    \Pr(X=0, Y=0) &= 1-p, \\
    \Pr(X=1, Y=y) &= \frac{e^{-p} p^y}{y!}, \qquad y = 1, 2, \dotsc, \\
    \Pr(X=1, Y=0) &= e^{-p} - (1-p), \\
    \Pr(X=x, Y=y) &= 0, \qquad \text{otherwise}.
\end{align*}

\begin{Parts}
    \Part Recall that all valid distributions satisfy two important properties. Argue that this distribution is a valid joint distribution.
	\begin{Answer}
     All probabilities are between 0 and 1. This is trivial for the first and last case. For the second case, as $y$ approaches infinity, $\Pr[X=1, Y=y]$ will approach 0. For the third case, as $p$ approaches 0, $\Pr[X=1, Y=0]$ approaches 0 and as $p$ approaches $1$, $\Pr[X=1, Y=0]$ will approach $\frac{1}{e}$. The second property is met because the sum of all probabilities equals 1.
    $$1-p+e^{-p}-(1-p)+\sum_{y=1}^{\infty}\frac{e^{-p}p^y}{y!}$$
    $$=e^{-p}+e^{-p}(e^{p}-1)$$
    $$=1$$
	\end{Answer}

    \Part Show that $X$ has the Bernoulli distribution with probability $p$.
	\begin{Answer}
    For a Bernoulli random variable $X$, $\Pr[X=k]=p^k(1-p)^{1-k}$ for $k \in {0,1}$. According to the joint distribution, $\Pr[X=0]=1-p$ and $\Pr[X=1]=e^{-p}-(1-p)+\sum_{y=1}^{\infty}\frac{e^{-p}p^y}{y!}=p$. Thus, $X$ has a Bernoulli distribution with probability $p$. 
	\end{Answer}

    \Part Show that $Y$ has the Poisson distribution with parameter $\lambda = p$.
	\begin{Answer}
    For a Poisson random variable $Y$, $\Pr[Y=k]=\frac{\lambda^ke^{-\lambda}}{k!}$ for $k \in \N$. According to the joint distribution, $Pr[Y=0]=1-p+e^{-p}-(1-p)=e^{-p}$. This is consistent with the general form of a Poisson random distribution; $\Pr[Y=0]= e^{-\lambda}$. For all other possible $y$, the joint distribution states that $\Pr[Y=y]=
\frac{e^{-p} p^y}{y!}$. This too matches the probabilities of a Poisson random variable. Thus, since all possible values of $y$ match the Poisson distribution, $Y$ has the Poisson distribution with parameter $\lambda=p$.
\end{Answer}

    \Part Show that $\Pr(X \neq Y) \leq p^2$.
	\begin{Answer}
    Alternatively, we can show that $1-\Pr[X=Y] \leq p^2$. This is equal to the following: $$1-(1-p+e^{-p}p)$$
    $$=p(1-e^{-p}) \leq p^2$$
    Thus, we must show that $(1-e^{-p})\leq p$. This is equivalent to showing $e^p(1-p) \leq 1$ which we know is true because $p$ is bounded by 0 and 1, inclusive. Thus, $\Pr(X \neq Y) \leq p^2$.
	\end{Answer}
\end{Parts}

Now, let $X_i$, $i = 1, 2, \dotsc$ be a sequence of random variables with probabilities $p_i$, $i = 1, 2, \dotsc$. Similarly, let $Y_i$ be a Poisson random variable with parameter $\lambda = p_i$, $i=1, 2, \dotsc$. The $X_i$ and $Y_i$ are coupled, so that they have the joint distribution described above (with $p = p_i$), but for $i \neq j$, $(X_i, Y_i)$ and $(X_j, Y_j)$ are independent.

We will now introduce a coupling argument which shows that the distribution of $\sum_{i=1}^n X_i$ approaches a Poisson distribution with parameter $\lambda = p_1 + \cdots + p_n$.

\begin{Parts}
    \setcounter{enumi}{4}
    \Part A common way to measure the ``distance'' between two probability distributions is known as the total variation norm, and it is given by
    \begin{align*}
        d(X, Y) &= \frac{1}{2} \sum_{k=0}^\infty |\Pr(X = k) - \Pr(Y = k)|.
    \end{align*}
    Show that $d(X, Y) \leq \Pr(X \neq Y)$. [\textit{Hint}: Use the Law of Total Probability to split up the events according to $\{X = Y\}$ and $\{X \neq Y\}$.]
	\begin{Answer}
    $$\frac{1}{2}\sum_{k=0}^{\infty}|{\Pr[X=k]-\Pr[Y=k]}|\leq \Pr(X \not = Y)$$ $$\frac{1}{2}\sum_{k=0}^{\infty}|\Pr[X=k, X \not = Y]-\Pr[Y=k, X=Y]| \leq 1-\sum_{k=0}^{\infty}\Pr[Y=k,X=k]$$ $$\frac{1}{2}\sum_{k=0}^{\infty}\Pr[X=k, X \not = Y]+\Pr[Y=k, X=Y] \leq 1-\sum_{k=0}^{\infty}\Pr[Y=k,X=k]$$ $$\frac{1}{2}\sum_{k=0}^{\infty}\Pr[X=k, X \not = Y]+\Pr[Y=k, X=Y]+2\Pr[Y=k, X=k] \leq 1$$ $$\frac{1}{2}(1+1) \leq 1$$
    $$1\leq 1$$
	\end{Answer}

    \Part Show that $\Pr(\sum_{i=1}^n X_i \neq \sum_{i=1}^n Y_i) \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)$. [\textit{Hint}: Maybe try the Union Bound.]
	\begin{Answer}
    Since we know $\Pr[\bigcup_{i=1}^n(X_i \not = Y_i)] \leq \sum_{i=1}^n \Pr[X_i \not = Y_i]$ by the Union bound, it is sufficient to prove that $\Pr[\bigcup_{i=1}^n(X_i \not = Y_i)] \geq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i]$. This is equivalent to the following: $$1-\Pr[\bigcup_{i=1}^n(X_i = Y_i)]\geq\Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i]$$ $$1-\Pr[\sum_{i=1}^n X_i = \sum_{i=1}^n Y_i]\geq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i]$$ $$1 \geq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i]+\Pr[\sum_{i=1}^n X_i = \sum_{i=1}^n Y_i]$$$$1\geq 1$$
	\end{Answer}

    \Part Finally, for the $X_i$ and $Y_i$ defined above, show that $d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \sum_{i=1}^n p_i^2$.
	\begin{Answer}
    Using parts e,f, and d, we know the following: $$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i]$$ $$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i] \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)$$ $$d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \Pr[\sum_{i=1}^n X_i \not = \sum_{i=1}^n Y_i] \leq \sum_{i=1}^n \Pr(X_i \neq Y_i)\leq \sum_{i=1}^n p_i^2$$
    Thus, $d(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i) \leq \sum_{i=1}^n p_i^2$.
	\end{Answer}
\end{Parts}
%%%%%%%%%%%%%%%%%%%% QUESTIONS END HERE

\end{document}